{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rap_db import *\n",
    "from rap_clean import *\n",
    "from rap_viz import line, verse_graph\n",
    "from nltk.stem import *\n",
    "from nltk import pos_tag\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from sklearn.model_selection import train_test_split as tr_ts_spl\n",
    "from sklearn.naive_bayes import MultinomialNB as multi_NB\n",
    "from collections import Counter as cntr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doom = art_load(['Doom'])['Doom']\n",
    "chief = art_load(['Chief Keef'])['Chief Keef']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "#verse count for each\n",
    "print(len(doom.uniq_art_verses))\n",
    "print(len(chief.uniq_art_verses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class line_data():\n",
    "    def __init__(self, in_line_obj):\n",
    "        self.line_obj = in_line_obj    \n",
    "        self.gen_line_stem()\n",
    "        self.gen_line_metrics()\n",
    "\n",
    "    #want to run in multinomial and bernouli ways (one with frequency one with there not there binary)\n",
    "    def gen_line_stem(self):\n",
    "        #no uniques, one with and without stems\n",
    "        #keeping a full version just in case\n",
    "        self.all_words = [w.lower() for w in self.line_obj.words_as_strings]\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        words_stm = [stemmer.stem(w) for w in self.all_words if stemmer.stem(w) not in stopwords.words('english')]\n",
    "        self.all_stemmed_words = list(filter(None, words_stm))\n",
    "        \n",
    "    #may want to add stemming if accuracy shitty\n",
    "    def gen_line_metrics(self):\n",
    "        #get the vowel strings needed\n",
    "        ex_vwls = self.line_obj.vowel_sounds\n",
    "        nr_vwls = [v[:2] for v in ex_vwls]\n",
    "        brd_vwls = [v[:1] for v in ex_vwls]\n",
    "        #then vowel sounds for unique words, do it this way to not remake word objects\n",
    "        check = set()\n",
    "        ex_vwls_uniqs = []\n",
    "        for w in self.line_obj.word_objs:\n",
    "            if w.text.lower() not in check:\n",
    "                check = check|{w.text.lower()}\n",
    "                ex_vwls_uniqs.extend(list(zip(*w.matches))[1])\n",
    "        nr_vwls_uniqs = [v[:2] for v in ex_vwls_uniqs]\n",
    "        brd_vwls_uniqs = [v[:1] for v in ex_vwls_uniqs]\n",
    "        \n",
    "        #these are used a lot\n",
    "        wrds = self.line_obj.words_as_strings\n",
    "        unq_wrds = self.line_obj.uniq_words_as_strings\n",
    "        wrd_cnt = len(wrds)\n",
    "        unq_wrd_cnt = len(unq_wrds)\n",
    "        blobs = TextBlob(\" \".join(wrds)).sentiment\n",
    "        \n",
    "        #word based metrics\n",
    "        self.metrics={'avg_wrd_len':sum(map(len,wrds))/wrd_cnt,\n",
    "        'avg_unq_wrd_len':sum(map(len,unq_wrds))/unq_wrd_cnt,\n",
    "        'unq_wrds_rat':unq_wrd_cnt/wrd_cnt,\n",
    "                      \n",
    "        #vowel based metrics\n",
    "            #average vowel sounds per word\n",
    "        'avg_wrd_vwls':len(ex_vwls)/wrd_cnt,\n",
    "            #average vowel sounds per unique word\n",
    "        'avg_unq_wrd_vwls':len(ex_vwls_uniqs)/unq_wrd_cnt,\n",
    "            #average unique vowel sounds per word\n",
    "        'avg_wrd_brd_unq_vwls':len(set(brd_vwls))/wrd_cnt,\n",
    "        'avg_wrd_nr_unq_vwls':len(set(nr_vwls))/wrd_cnt,\n",
    "        'avg_wrd_ex_unq_vwls':len(set(ex_vwls))/wrd_cnt,\n",
    "            #average unique vowel sounds per unique word\n",
    "        'avg_unq_wrd_brd_unq_vwls':len(set(brd_vwls_uniqs))/unq_wrd_cnt,\n",
    "        'avg_unq_wrd_nr_unq_vwls':len(set(nr_vwls_uniqs))/unq_wrd_cnt,\n",
    "        'avg_unq_wrd_ex_unq_vwls':len(set(ex_vwls_uniqs))/unq_wrd_cnt,\n",
    "                      \n",
    "        #specialized metrics\n",
    "        'pol':blobs.polarity,\n",
    "        'subj':blobs.subjectivity,\n",
    "        'uniq_pos_rat': len(set(list(zip(*pos_tag(wrds)))[1]))/wrd_cnt,\n",
    "        'uniq_pos_unq_wrd_rat': len(set(list(zip(*pos_tag(unq_wrds)))[1]))/unq_wrd_cnt}\n",
    "        return self.metrics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corr_plot(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def artist_to_data(art_obj, inp_pop=False, inp_exc_line=True, inp_opto_type='near'):#opto stuff\n",
    "    #sample to get keys for line data metrics, not needed later\n",
    "    _ld = line_data(verse_graph(art_obj.uniq_art_verses[0], '', '').ver_as_lines[0])\n",
    "    metric_df = pd.DataFrame(columns = list(_ld.gen_line_metrics())+['artist'])\n",
    "    lingustic_df = pd.DataFrame(columns=['artist','text','all_text'])\n",
    "    \n",
    "    for v in art_obj.uniq_art_verses:\n",
    "        verse_g = verse_graph(v, art_obj.name, '')\n",
    "        verse_g.opto_matches(pop=inp_pop, exc_line=inp_exc_line, opto_type=inp_opto_type, record=False)#opto verse\n",
    "\n",
    "        for v_line in verse_g.ver_as_lines:\n",
    "            #if the line has actually registered words\n",
    "            if v_line.word_objs:\n",
    "                #make a line_data object from the line object\n",
    "                line_data_obj = line_data(v_line)\n",
    "                app_dic = copy(line_data_obj.metrics)\n",
    "                app_dic.update({'artist':art_obj.name})#setup the dictionaries to feed to DF\n",
    "                metric_df = metric_df.append(app_dic, ignore_index=True)\n",
    "                #special lingustic data (make)\n",
    "                lingustic_df = lingustic_df.append({'text':line_data_obj.all_stemmed_words, 'all_text':line_data_obj.all_words, 'artist':art_obj.name}, ignore_index=True)  \n",
    "    return metric_df, lingustic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_validate_split(art_lines, rs1=42, rs2=41):\n",
    "    y = art_lines['artist']\n",
    "    x = art_lines.ix[:, art_lines.columns.difference(['artist'])]\n",
    "    x_tr, _x_ts, y_tr, _y_ts = tr_ts_spl(x, y, test_size=0.4, random_state=rs1)\n",
    "    x_ts, x_vl, y_ts, y_vl = tr_ts_spl(_x_ts, _y_ts, test_size=0.5, random_state=rs2)\n",
    "    return {'x_train':x_tr,'y_train':y_tr,'x_test':x_ts,'y_test':y_ts,'x_val':x_vl,'y_val':y_vl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_met, d_lin = artist_to_data(doom, inp_pop=2, inp_exc_line=False, inp_opto_type='exact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_met, c_lin = artist_to_data(chief, inp_pop=False, inp_exc_line=True, inp_opto_type='near')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvenuti\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning:\n",
      "\n",
      "\n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comb_met_data = train_test_validate_split(d_met.append(c_met))\n",
    "comb_lin_data = train_test_validate_split(d_lin.append(c_lin))\n",
    "https://docs.python.org/dev/library/collections.html#collections.Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66018423746161714"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_mod = SVC()\n",
    "svm_mod.fit(comb_met_data['x_train'], comb_met_data['y_train'])\n",
    "svm_mod.score(comb_met_data['x_test'], comb_met_data['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you did\n",
    "built basic data sorting functions, ran SVM with no tuning 67% accuracy\n",
    "\n",
    "\n",
    "What you need to do next\n",
    "seet up lingusitc data set trainngi\n",
    "make corr plot\n",
    "\n",
    "Notes\n",
    "\n",
    "Long term\n",
    "Train models using two different training methadologies\n",
    "1. text bag of words (simply look at words in textand classify using a naive bayes, random forest, SVM)\n",
    "2. make a row for every line based on the whiteboarded lingustic measures (def use svm, maybe random forest, maybe KNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
